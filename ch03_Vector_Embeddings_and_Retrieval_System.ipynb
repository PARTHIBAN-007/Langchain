{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Vector Stores and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "documents = [\"Langchain is used for Building LLM Applications\", \"Hugging Face is for transformers and LLM \", \"Agentic AI can be built using langchain and langgraph\"]\n",
    "\n",
    "embeddings = embedding_model.encode(documents)\n",
    "\n",
    "embeddings[0].shape # Dimension of each sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = embeddings.shape[0]  \n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(np.array(embeddings, dtype=np.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.72871506 1.3303838 ]] [[0 2]]\n"
     ]
    }
   ],
   "source": [
    "def search(query,top_k=2):\n",
    "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
    "    distance , indices = index.search(query_embedding,top_k)\n",
    "    print(distance , indices) \n",
    "    results = [(documents[i],distance[0][j]) for j ,i in enumerate(indices[0]) ]\n",
    "    return results\n",
    "\n",
    "query = \"What is called Langchain\"\n",
    "results = search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Langchain is used for Building LLM Applications | Distance: 0.7287\n",
      "Document: Agentic AI can be built using langchain and langgraph | Distance: 1.3304\n"
     ]
    }
   ],
   "source": [
    "# Euclidean : Lower Score - Higher Similarity , Higher Score - Lower Similarity\n",
    "for result , distance in results:\n",
    "    print(f\"Document: {result} | Distance: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Langchain is used for Building LLM Applications | Inner Product Similarity: 0.6356\n",
      "Document: Agentic AI can be built using langchain and langgraph | Inner Product Similarity: 0.3348\n"
     ]
    }
   ],
   "source": [
    "index_ip = faiss.IndexFlatIP(d)\n",
    "index_ip.add(embeddings)\n",
    "\n",
    "def search_ip(query, top_k=2):\n",
    "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
    "    distances, indices = index_ip.search(query_embedding, top_k)\n",
    "    return [(documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "\n",
    "query = \"What is called Langchain\"\n",
    "results = search_ip(query)\n",
    "\n",
    "# Dot Product : Higher Score - Higher Similarity , Lower Score - Lower Similarity\n",
    "for doc, dist in results:\n",
    "    print(f\"Document: {doc} | Inner Product Similarity: {dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Langchain is used for Building LLM Applications | Cosine Similarity: 0.6356\n",
      "Document: Agentic AI can be built using langchain and langgraph | Cosine Similarity: 0.3348\n"
     ]
    }
   ],
   "source": [
    "faiss.normalize_L2(embeddings)\n",
    "index_cosine = faiss.IndexFlatIP(d)  \n",
    "index_cosine.add(embeddings)\n",
    "\n",
    "def search_cosine(query, top_k=2):\n",
    "    query_embedding = embedding_model.encode([query]).astype(np.float32)\n",
    "    faiss.normalize_L2(query_embedding)  \n",
    "    distances, indices = index_cosine.search(query_embedding, top_k)\n",
    "    return [(documents[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "\n",
    "query = \"What is called Langchain\"\n",
    "results = search_cosine(query)\n",
    "# Cosine Similarity : Higher Score - Higher Similarity , Lower Score - Lower Similarity\n",
    "\n",
    "for doc, dist in results:\n",
    "    print(f\"Document: {doc} | Cosine Similarity: {dist:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
