{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import google.generativeai as genai\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_five(x):\n",
    "    return x+5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x*2\n",
    "\n",
    "add_five = RunnableLambda(add_five)\n",
    "multiply_by_two = RunnableLambda(multiply_by_two)\n",
    "chain = add_five | multiply_by_two\n",
    "chain.invoke(5) # 5+5 = 10 -> 10*2 = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story Result: Captain Elara Vance, her face a roadmap of wrinkles etched by solar flares and zero-g, stared out at Xylos. The planet, a swirling turquoise and emerald behemoth, pulsed with a malevolent energy.  Xylos was known as the Graveyard of Starships – a beautiful death trap.  Her mission: retrieve the black box from the *Stardust*, a vessel swallowed by Xylos's unpredictable gravitational tides a decade ago.  No one had returned.\n",
      "\n",
      "Elara wasn't afraid. Fear was a luxury she'd shed years ago, orbiting Jupiter's storm clouds.  She strapped herself into the *Sparrow*, her nimble, single-person exploration craft, its hull shimmering with a newly developed anti-graviton shield.  Xylos's gravity was a chaotic beast, tearing at the fabric of spacetime.  Even the Sparrow shuddered as it plunged into the turbulent atmosphere.\n",
      "\n",
      "The *Stardust*, half-buried in a crystalline forest that shimmered with an unnerving bioluminescence, was a mangled wreck.  Elara navigated the treacherous terrain, dodging razor-sharp crystalline flora and avoiding chasms that spat out bursts of Xylos's strange, energy-charged plasma.  She felt the Sparrow's hull groan under the pressure, the anti-graviton shield flickering warningly.\n",
      "\n",
      "Inside the *Stardust*, the air hung thick with the smell of decay and ozone.  Skeletal remains of the crew were scattered amongst broken consoles.  But Elara pressed on, her focus unwavering.  She located the black box – a battered, obsidian cube humming with faint energy.\n",
      "\n",
      "Suddenly, the ground trembled.  A colossal, bioluminescent creature, its form shifting like liquid light, emerged from the crystalline forest.  Its eyes, twin orbs of pure, malevolent energy, fixed on Elara.  This was Xylos's guardian, a being of pure, hostile energy.\n",
      "\n",
      "Elara didn't flinch.  She knew escape was impossible.  Instead, she activated the Sparrow's emergency beacon, a desperate gamble to alert the waiting fleet.  Then, she turned to face the creature, her hand resting on the Sparrow's self-destruct button.  She wouldn't let Xylos claim another victim.  Her sacrifice, her bravery, would be a warning to others.  The creature lunged – and the Sparrow vanished in a blinding flash of light.  The beacon screamed into the void.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = GoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\",\"style\"],\n",
    "     template= \"Tell me a short story about {topic} in {style} style\"\n",
    ")\n",
    "\n",
    "story_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = story_chain.invoke({\n",
    "    \"topic\": \"a brave astronaut\",\n",
    "    \"style\": \"science fiction\"\n",
    "})\n",
    "\n",
    "print(\"Story Result:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Book : title='Deep Learning with Python' topic='Large Language Model' author='François Chollet' reason='While not exclusively focused on LLMs, this book provides a strong foundation in deep learning principles, which are crucial for understanding the inner workings of large language models.  It covers relevant topics like recurrent neural networks and attention mechanisms, essential components of LLMs.  The practical examples and clear explanations make it accessible to a wide audience.'\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel , Field\n",
    "\n",
    "class BookRecommendation(BaseModel):\n",
    "    title : str = Field(description=\"The Book Title\")\n",
    "    topic : str = Field(description=\"Topic of the book\")\n",
    "    author : str = Field(description=\"Author of the Book\")\n",
    "    reason : str = Field(description=\"Why this Book is recommended\")\n",
    "\n",
    "\n",
    "book_parser = PydanticOutputParser(pydantic_object=BookRecommendation)\n",
    "\n",
    "book_prompt = PromptTemplate(input_variables=[\"Topic\",\"format_instructions\"],\n",
    "                        template=\"Recommend a Book based on the following preferences Topic : {Topic} {format_instructions} \")\n",
    "\n",
    "book_chain = book_prompt | llm | book_parser\n",
    "\n",
    "response = book_chain.invoke({\"Topic\":\"Large Language Model\",\"format_instructions\": book_parser.get_format_instructions()})\n",
    "print(\"Recommended Book :\",response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
